{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.constraints import unit_norm\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow.keras.backend as K\n",
    "import random\n",
    "import os\n",
    "import logmelspectr_params as params\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint_path='/nas/home/cborrelli/tripletloss_bot/checkpoints/vggish/vggish_model.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path)\n",
    "var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "\n",
    "for key in var_to_shape_map:\n",
    "    print(\"tensor_name: \", key)\n",
    "    print(reader.get_tensor(key).shape) # Remove this is you want to print only variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=-1, keepdims=True), K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model(input_shape):\n",
    "\n",
    "    img_input = Input(shape=input_shape)\n",
    "    \n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv1')(img_input)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(x)\n",
    "    \n",
    "    # Block fc\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc1_1')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc1_2')(x)\n",
    "    \n",
    "    model = Model(img_input, x, name='vggish')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_model(embedding_shape):\n",
    "    embedding_l = Input(embedding_shape, name='embed_ref')\n",
    "    embedding_r = Input(embedding_shape, name='embed_dif')\n",
    "    lambda_layer  = Lambda(euclidean_distance)([embedding_l, embedding_r])\n",
    "    model = Model([embedding_l, embedding_r], lambda_layer, name='distance')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_siamese_network(input_shape, \n",
    "                           checkpoint_path='/nas/home/cborrelli/tripletloss_bot/checkpoints/vggish/vggish_model.ckpt'):\n",
    "    \"\"\"\n",
    "    Create the siamese model structure using the supplied base and head model.\n",
    "    \"\"\"\n",
    "    input_ref = Input(input_shape, name=\"input_ref\") # reference track\n",
    "    input_dif = Input(input_shape, name=\"input_dif\") # different track\n",
    "\n",
    "    base_model = get_base_model(input_shape)\n",
    "    \n",
    "    \n",
    "    # Initialize base model with VGGish weights\n",
    "    reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path)\n",
    "    var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "\n",
    "    tensor_layers_list = []\n",
    "    for key in var_to_shape_map:    \n",
    "        tensor_layers_list.append('/'.join(key.split('/')[:-1]))\n",
    "\n",
    "    for index, t in enumerate(tensor_layers_list):\n",
    "        weights_key = t + '/weights'\n",
    "        bias_key = t + '/biases'\n",
    "        weights = reader.get_tensor(weights_key)\n",
    "        biases = reader.get_tensor(bias_key)\n",
    "\n",
    "        keras_layer_name = t.split('/')[-1]\n",
    "        if keras_layer_name=='logits' or keras_layer_name == 'fc2':\n",
    "            continue\n",
    "  \n",
    "        base_model.get_layer(keras_layer_name).set_weights([weights, biases])\n",
    "\n",
    "\n",
    "    \n",
    "    processed_ref = base_model(input_ref)\n",
    "    processed_dif = base_model(input_dif)\n",
    "\n",
    "    head_model = get_head_model(base_model.output_shape[-1])\n",
    "    head = head_model([processed_ref, processed_dif])\n",
    "\n",
    "    siamese_model = Model([input_ref, input_dif], head)\n",
    "    return siamese_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = create_siamese_network(input_shape=(96, 64, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display       import SVG\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, \n",
    "                 show_shapes      = True, \n",
    "                 show_layer_names = True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gombru.github.io/2019/04/03/ranking_loss/\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    return K.mean((1 - y_true) * K.square(y_pred) + (y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame(data, window_length, hop_length):\n",
    "    \"\"\"Convert array into a sequence of successive possibly overlapping frames.\n",
    "    An n-dimensional array of shape (num_samples, ...) is converted into an\n",
    "    (n+1)-D array of shape (num_frames, window_length, ...), where each frame\n",
    "    starts hop_length points after the preceding one.\n",
    "    This is accomplished using stride_tricks, so the original data is not\n",
    "    copied.  However, there is no zero-padding, so any incomplete frames at the\n",
    "    end are not included.\n",
    "    Args:\n",
    "    data: np.array of dimension N >= 1.\n",
    "    window_length: Number of samples in each frame.\n",
    "    hop_length: Advance (in samples) between each window.\n",
    "    Returns:\n",
    "    (N+1)-D np.array with as many rows as there are complete frames that can be\n",
    "    extracted.\n",
    "    \"\"\"\n",
    "    num_samples = data.shape[0]\n",
    "    num_frames = 1 + int(np.floor((num_samples - window_length) / hop_length))\n",
    "    shape = (num_frames, window_length) + data.shape[1:]\n",
    "    strides = (data.strides[0] * hop_length,) + data.strides\n",
    "    result = np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n",
    "    return result\n",
    "\n",
    "class TrainDataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    \n",
    "    def __init__(self, dataframe, feature_path, batch_size=32, dim=(96, 64), n_channels=1,\n",
    "                  shuffle=True, classes_list=['-', 'A01', 'A02', 'A03', 'A04', 'A05', 'A06'],\n",
    "                num_batch_epoch=100):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.dataframe = dataframe\n",
    "        self.classes_list = classes_list\n",
    "        self.n_channels = n_channels\n",
    "        self.len = num_batch_epoch\n",
    "        self.feature_path = feature_path\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, batch_index):\n",
    "        'Generate one batch of data'        \n",
    "        negative_couples_classes = np.array(list(itertools.combinations(self.classes_list, r=2)))\n",
    "        positive_couples_classes = np.array(list(zip(self.classes_list, self.classes_list)))\n",
    "\n",
    "        negative_selected_pairs = negative_couples_classes[np.random.choice(negative_couples_classes.shape[0], \n",
    "                                                            self.batch_size // 2, replace=True), :]\n",
    "        positive_selected_pairs = positive_couples_classes[np.random.choice(positive_couples_classes.shape[0], \n",
    "                                                            self.batch_size // 2, replace=True), :]\n",
    "\n",
    "        selected_pairs = np.concatenate((positive_selected_pairs, negative_selected_pairs), axis=0)\n",
    "\n",
    "        y = np.concatenate((np.zeros((self.batch_size//2)), np.ones((self.batch_size//2))), axis=0)\n",
    "\n",
    "        features_sample_rate = 1.0 / params.STFT_HOP_LENGTH_SECONDS\n",
    "\n",
    "        example_window_length = int(round(\n",
    "            params.EXAMPLE_WINDOW_SECONDS * features_sample_rate))\n",
    "        example_hop_length = int(round(\n",
    "            params.EXAMPLE_HOP_SECONDS * features_sample_rate))\n",
    "\n",
    "        X_0 = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        X_1 = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "\n",
    "        for sample_batch_index, pairs in enumerate(selected_pairs):\n",
    "\n",
    "            sample = np.empty((2, *self.dim, self.n_channels))\n",
    "            for a, alg in enumerate(pairs):\n",
    "                row = self.dataframe[self.dataframe.system_id == alg].sample(n=1)\n",
    "                log_mel = np.load(os.path.join(self.feature_path, row['audio_filename'].values[0] + '.npy'))\n",
    "                log_mel = log_mel.transpose()\n",
    "\n",
    "                if log_mel.shape[0] < self.dim[0]:\n",
    "                    pad_len = self.dim[0] - log_mel.shape[0] + 1\n",
    "                    log_mel = np.pad(log_mel, ((0, pad_len), (0, 0)))\n",
    "\n",
    "                log_mel = frame(log_mel, example_window_length, example_hop_length)\n",
    "\n",
    "                selected_frame = np.random.randint(low=0, high=log_mel.shape[0], size=1)\n",
    "\n",
    "                selected_log_mel = log_mel[selected_frame, :, :]\n",
    "                selected_log_mel = selected_log_mel[0,:, :, np.newaxis]\n",
    "\n",
    "                sample[a] = selected_log_mel\n",
    "\n",
    "            X_0[sample_batch_index] = sample[0]            \n",
    "            X_1[sample_batch_index] = sample[1]            \n",
    "\n",
    "\n",
    "        return [X_0, X_1], y\n",
    "\n",
    "    \n",
    "    \n",
    "class TestDataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    \n",
    "    def __init__(self, dataframe, feature_path, batch_size=32, dim=(96, 64), n_channels=1,\n",
    "                  shuffle=True, classes_pair=['-', '-'],\n",
    "                num_batch_epoch=100):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.dataframe = dataframe\n",
    "        self.n_channels = n_channels\n",
    "        self.len = num_batch_epoch\n",
    "        self.feature_path = feature_path\n",
    "        self.classes_pair = classes_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, batch_index):\n",
    "        'Generate one batch of data'        \n",
    "\n",
    "            # If i am specifying only one element it means I want to use the data generator for testing\n",
    "            # only one class\n",
    "        \n",
    "        selected_pairs =  [self.classes_pair] * self.batch_size\n",
    "        features_sample_rate = 1.0 / params.STFT_HOP_LENGTH_SECONDS\n",
    "        example_window_length = int(round(\n",
    "            params.EXAMPLE_WINDOW_SECONDS * features_sample_rate))\n",
    "        example_hop_length = int(round(\n",
    "            params.EXAMPLE_HOP_SECONDS * features_sample_rate))\n",
    "\n",
    "        X_0 = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        X_1 = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        \n",
    "        if self.classes_pair[0] == self.classes_pair[1]:\n",
    "            y = np.zeros((self.batch_size))\n",
    "        else:\n",
    "            y = np.ones((self.batch_size))\n",
    "            \n",
    "        for sample_batch_index, pairs in enumerate(selected_pairs):\n",
    "\n",
    "            sample = np.empty((2, *self.dim, self.n_channels))\n",
    "            for a, alg in enumerate(pairs):\n",
    "                row = self.dataframe[self.dataframe.system_id == alg].sample(n=1)\n",
    "                log_mel = np.load(os.path.join(self.feature_path, row['audio_filename'].values[0] + '.npy'))\n",
    "                log_mel = log_mel.transpose()\n",
    "\n",
    "                if log_mel.shape[0] < self.dim[0]:\n",
    "                    pad_len = self.dim[0] - log_mel.shape[0] + 1\n",
    "                    log_mel = np.pad(log_mel, ((0, pad_len), (0, 0)))\n",
    "\n",
    "                log_mel = frame(log_mel, example_window_length, example_hop_length)\n",
    "\n",
    "                selected_frame = np.random.randint(low=0, high=log_mel.shape[0], size=1)\n",
    "\n",
    "                selected_log_mel = log_mel[selected_frame, :, :]\n",
    "                selected_log_mel = selected_log_mel[0,:, :, np.newaxis]\n",
    "\n",
    "                sample[a] = selected_log_mel\n",
    "\n",
    "            X_0[sample_batch_index] = sample[0]            \n",
    "            X_1[sample_batch_index] = sample[1]   \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        return [X_0, X_1], y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt_path = \"/nas/public/dataset/asvspoof2019/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "train_feat_root = \"/nas/home/cborrelli/tripletloss_bot/features/logmelspectr/train\"\n",
    "df_train = pd.read_csv(train_txt_path, sep=\" \", header=None)\n",
    "df_train.columns = [\"speaker_id\", \"audio_filename\", \"null\", \"system_id\", \"label\"]\n",
    "df_train = df_train.drop(columns=\"null\")\n",
    "\n",
    "dev_txt_path = \"/nas/public/dataset/asvspoof2019/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\"\n",
    "dev_feat_root = \"/nas/home/cborrelli/tripletloss_bot/features/logmelspectr/dev\"\n",
    "df_dev = pd.read_csv(dev_txt_path, sep=\" \", header=None)\n",
    "df_dev.columns = [\"speaker_id\", \"audio_filename\", \"null\", \"system_id\", \"label\"]\n",
    "df_dev = df_dev.drop(columns=\"null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "steps_per_epoch = 15\n",
    "\n",
    "\n",
    "train_classes_list = ['-', 'A01', 'A02', 'A03']\n",
    "\n",
    "train_generator = TrainDataGenerator(dataframe=df_train, feature_path=train_feat_root,\n",
    "                                     classes_list=train_classes_list, num_batch_epoch=steps_per_epoch)\n",
    "val_generator = TrainDataGenerator(dataframe=df_dev, feature_path=dev_feat_root, classes_list=train_classes_list,\n",
    "                                   num_batch_epoch=steps_per_epoch)\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='/nas/home/cborrelli/tripletloss_bot/checkpoints/siamese'),\n",
    "]\n",
    "## add callback\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(loss=contrastive_loss, optimizer=opt)\n",
    "\n",
    "history = model.fit(train_generator, validation_data=val_generator,\n",
    "                    epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that save model is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "steps_per_epoch = 15\n",
    "\n",
    "\n",
    "train_classes_list = ['-', 'A01', 'A02', 'A03']\n",
    "\n",
    "train_generator = TrainDataGenerator(dataframe=df_train, feature_path=train_feat_root, batch_size=1,\n",
    "                                     classes_list=train_classes_list, num_batch_epoch=steps_per_epoch)\n",
    "val_generator = TrainDataGenerator(dataframe=df_dev, feature_path=dev_feat_root, classes_list=train_classes_list,\n",
    "                                   batch_size=1,\n",
    "                                   num_batch_epoch=steps_per_epoch)\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='/nas/home/cborrelli/tripletloss_bot/checkpoints/siamese'),\n",
    "]\n",
    "## add callback\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(loss=contrastive_loss, optimizer=opt)\n",
    "\n",
    "history = model.fit(train_generator, validation_data=val_generator,\n",
    "                    epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = TrainDataGenerator(dataframe=df_train, feature_path=train_feat_root, batch_size=1,\n",
    "                                     classes_list=train_classes_list, num_batch_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
